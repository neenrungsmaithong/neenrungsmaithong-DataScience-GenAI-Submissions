{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neenrungsmaithong/neenrungsmaithong-DataScience-GenAI-Submissions/blob/main/Assignment_5/6_02_DNN_101_COMPLETED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "a2fa54a8-3ad5-4d52-c984-3230bd910069"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "38277092-7c86-4e29-958c-fdc70fddf343"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "9232c7ea-f8e7-46cd-ba2d-ebff76a18e36"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "1123f0da-1d03-4af4-b198-4a692df0ad08"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 29062.2734\n",
            "Epoch [10/100], Loss: 28887.1738\n",
            "Epoch [10/100], Loss: 32306.8301\n",
            "Epoch [10/100], Loss: 33756.0469\n",
            "Epoch [10/100], Loss: 22366.9473\n",
            "Epoch [10/100], Loss: 28772.0039\n",
            "Epoch [10/100], Loss: 32817.3125\n",
            "Epoch [10/100], Loss: 17422.5957\n",
            "Epoch [20/100], Loss: 34676.9883\n",
            "Epoch [20/100], Loss: 30719.5039\n",
            "Epoch [20/100], Loss: 29589.8828\n",
            "Epoch [20/100], Loss: 27565.9238\n",
            "Epoch [20/100], Loss: 30950.6309\n",
            "Epoch [20/100], Loss: 26477.7168\n",
            "Epoch [20/100], Loss: 26409.0742\n",
            "Epoch [20/100], Loss: 34643.3867\n",
            "Epoch [30/100], Loss: 25893.8535\n",
            "Epoch [30/100], Loss: 29622.7344\n",
            "Epoch [30/100], Loss: 28370.207\n",
            "Epoch [30/100], Loss: 27093.6113\n",
            "Epoch [30/100], Loss: 27594.3594\n",
            "Epoch [30/100], Loss: 33831.4961\n",
            "Epoch [30/100], Loss: 32909.2227\n",
            "Epoch [30/100], Loss: 38392.8203\n",
            "Epoch [40/100], Loss: 32170.6992\n",
            "Epoch [40/100], Loss: 25066.7188\n",
            "Epoch [40/100], Loss: 31469.9688\n",
            "Epoch [40/100], Loss: 31056.5215\n",
            "Epoch [40/100], Loss: 23413.4785\n",
            "Epoch [40/100], Loss: 31981.9102\n",
            "Epoch [40/100], Loss: 30665.5371\n",
            "Epoch [40/100], Loss: 9177.3965\n",
            "Epoch [50/100], Loss: 22821.2598\n",
            "Epoch [50/100], Loss: 26589.209\n",
            "Epoch [50/100], Loss: 34079.8945\n",
            "Epoch [50/100], Loss: 32062.2852\n",
            "Epoch [50/100], Loss: 27561.4844\n",
            "Epoch [50/100], Loss: 30228.8086\n",
            "Epoch [50/100], Loss: 30528.0352\n",
            "Epoch [50/100], Loss: 12294.5029\n",
            "Epoch [60/100], Loss: 24799.0938\n",
            "Epoch [60/100], Loss: 24520.0215\n",
            "Epoch [60/100], Loss: 33418.7031\n",
            "Epoch [60/100], Loss: 23803.8535\n",
            "Epoch [60/100], Loss: 28212.2969\n",
            "Epoch [60/100], Loss: 37717.1719\n",
            "Epoch [60/100], Loss: 27044.9785\n",
            "Epoch [60/100], Loss: 46164.8438\n",
            "Epoch [70/100], Loss: 25583.4844\n",
            "Epoch [70/100], Loss: 33200.4375\n",
            "Epoch [70/100], Loss: 26037.3145\n",
            "Epoch [70/100], Loss: 29690.25\n",
            "Epoch [70/100], Loss: 30070.1914\n",
            "Epoch [70/100], Loss: 26299.1289\n",
            "Epoch [70/100], Loss: 27680.7891\n",
            "Epoch [70/100], Loss: 12234.3359\n",
            "Epoch [80/100], Loss: 29596.4785\n",
            "Epoch [80/100], Loss: 26545.2793\n",
            "Epoch [80/100], Loss: 27681.7148\n",
            "Epoch [80/100], Loss: 27059.5586\n",
            "Epoch [80/100], Loss: 30985.9688\n",
            "Epoch [80/100], Loss: 24309.7598\n",
            "Epoch [80/100], Loss: 29007.2402\n",
            "Epoch [80/100], Loss: 5927.0762\n",
            "Epoch [90/100], Loss: 29285.6641\n",
            "Epoch [90/100], Loss: 29193.7734\n",
            "Epoch [90/100], Loss: 26891.0938\n",
            "Epoch [90/100], Loss: 24470.3438\n",
            "Epoch [90/100], Loss: 23092.8125\n",
            "Epoch [90/100], Loss: 25257.2598\n",
            "Epoch [90/100], Loss: 31727.709\n",
            "Epoch [90/100], Loss: 19513.918\n",
            "Epoch [100/100], Loss: 26017.1543\n",
            "Epoch [100/100], Loss: 27140.1973\n",
            "Epoch [100/100], Loss: 25515.8945\n",
            "Epoch [100/100], Loss: 22663.5469\n",
            "Epoch [100/100], Loss: 31094.4238\n",
            "Epoch [100/100], Loss: 24971.1992\n",
            "Epoch [100/100], Loss: 26370.5273\n",
            "Epoch [100/100], Loss: 35384.0078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "19977415-f871-4127-8126-c44cc194a831"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 22991.6376953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "e23557ad-67b9-4bff-e957-15684ac4a6ab"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0   12.072695   219.0\n",
              "1   11.054191    70.0\n",
              "2   11.928183   202.0\n",
              "3   13.597708   230.0\n",
              "4   11.674080   111.0\n",
              "..        ...     ...\n",
              "84  10.606114   153.0\n",
              "85   9.940240    98.0\n",
              "86   9.336263    37.0\n",
              "87   9.687247    63.0\n",
              "88  10.745776   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7038ab38-e515-4b0b-b4f4-1a98d2aa14d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12.072695</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11.054191</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11.928183</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.597708</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11.674080</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>10.606114</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>9.940240</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>9.336263</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>9.687247</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>10.745776</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7038ab38-e515-4b0b-b4f4-1a98d2aa14d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7038ab38-e515-4b0b-b4f4-1a98d2aa14d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7038ab38-e515-4b0b-b4f4-1a98d2aa14d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_4750e5eb-57df-479e-bf44-7b5d2658423e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4750e5eb-57df-479e-bf44-7b5d2658423e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          11.562580108642578,\n          10.518776893615723,\n          11.525137901306152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 1000 # changed to 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lslDrcr9cpN9",
        "outputId": "f5d6da30-1d59-4b5c-be7a-2c6acf667435"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 23888.3848\n",
            "Epoch [10/1000], Loss: 25946.2188\n",
            "Epoch [10/1000], Loss: 22669.2617\n",
            "Epoch [10/1000], Loss: 26980.375\n",
            "Epoch [10/1000], Loss: 24135.7754\n",
            "Epoch [10/1000], Loss: 26618.6387\n",
            "Epoch [10/1000], Loss: 25947.6445\n",
            "Epoch [10/1000], Loss: 60763.7461\n",
            "Epoch [20/1000], Loss: 22692.8477\n",
            "Epoch [20/1000], Loss: 32333.8438\n",
            "Epoch [20/1000], Loss: 21133.5527\n",
            "Epoch [20/1000], Loss: 22339.0469\n",
            "Epoch [20/1000], Loss: 27724.1738\n",
            "Epoch [20/1000], Loss: 23390.3672\n",
            "Epoch [20/1000], Loss: 21070.6016\n",
            "Epoch [20/1000], Loss: 41807.2227\n",
            "Epoch [30/1000], Loss: 15919.9697\n",
            "Epoch [30/1000], Loss: 19989.4004\n",
            "Epoch [30/1000], Loss: 26637.7891\n",
            "Epoch [30/1000], Loss: 25939.0352\n",
            "Epoch [30/1000], Loss: 23482.7617\n",
            "Epoch [30/1000], Loss: 21357.7637\n",
            "Epoch [30/1000], Loss: 31788.4336\n",
            "Epoch [30/1000], Loss: 11758.3496\n",
            "Epoch [40/1000], Loss: 22999.2773\n",
            "Epoch [40/1000], Loss: 21310.4023\n",
            "Epoch [40/1000], Loss: 19154.7988\n",
            "Epoch [40/1000], Loss: 28312.5645\n",
            "Epoch [40/1000], Loss: 25186.3086\n",
            "Epoch [40/1000], Loss: 20898.459\n",
            "Epoch [40/1000], Loss: 18835.7988\n",
            "Epoch [40/1000], Loss: 17564.0586\n",
            "Epoch [50/1000], Loss: 25381.2695\n",
            "Epoch [50/1000], Loss: 20140.8301\n",
            "Epoch [50/1000], Loss: 16058.0\n",
            "Epoch [50/1000], Loss: 21542.707\n",
            "Epoch [50/1000], Loss: 19528.2324\n",
            "Epoch [50/1000], Loss: 22910.5137\n",
            "Epoch [50/1000], Loss: 22420.7754\n",
            "Epoch [50/1000], Loss: 19976.5098\n",
            "Epoch [60/1000], Loss: 19054.2949\n",
            "Epoch [60/1000], Loss: 17514.5273\n",
            "Epoch [60/1000], Loss: 19557.9805\n",
            "Epoch [60/1000], Loss: 21017.3691\n",
            "Epoch [60/1000], Loss: 24289.6289\n",
            "Epoch [60/1000], Loss: 18579.6445\n",
            "Epoch [60/1000], Loss: 19785.9844\n",
            "Epoch [60/1000], Loss: 6820.6538\n",
            "Epoch [70/1000], Loss: 19255.0566\n",
            "Epoch [70/1000], Loss: 17429.7012\n",
            "Epoch [70/1000], Loss: 12460.6797\n",
            "Epoch [70/1000], Loss: 19698.8574\n",
            "Epoch [70/1000], Loss: 19016.9648\n",
            "Epoch [70/1000], Loss: 19794.1172\n",
            "Epoch [70/1000], Loss: 22111.1387\n",
            "Epoch [70/1000], Loss: 22163.8203\n",
            "Epoch [80/1000], Loss: 18180.8184\n",
            "Epoch [80/1000], Loss: 20201.0039\n",
            "Epoch [80/1000], Loss: 14627.5273\n",
            "Epoch [80/1000], Loss: 17046.0352\n",
            "Epoch [80/1000], Loss: 14745.5459\n",
            "Epoch [80/1000], Loss: 14651.5938\n",
            "Epoch [80/1000], Loss: 19913.3789\n",
            "Epoch [80/1000], Loss: 38323.75\n",
            "Epoch [90/1000], Loss: 17434.1465\n",
            "Epoch [90/1000], Loss: 13302.002\n",
            "Epoch [90/1000], Loss: 16087.3613\n",
            "Epoch [90/1000], Loss: 15282.3096\n",
            "Epoch [90/1000], Loss: 15985.9736\n",
            "Epoch [90/1000], Loss: 18046.3613\n",
            "Epoch [90/1000], Loss: 16163.4072\n",
            "Epoch [90/1000], Loss: 2437.2551\n",
            "Epoch [100/1000], Loss: 15606.6035\n",
            "Epoch [100/1000], Loss: 15710.4697\n",
            "Epoch [100/1000], Loss: 15516.416\n",
            "Epoch [100/1000], Loss: 16127.4775\n",
            "Epoch [100/1000], Loss: 17495.2969\n",
            "Epoch [100/1000], Loss: 10107.2383\n",
            "Epoch [100/1000], Loss: 12574.1699\n",
            "Epoch [100/1000], Loss: 4698.2837\n",
            "Epoch [110/1000], Loss: 11182.5918\n",
            "Epoch [110/1000], Loss: 12258.6758\n",
            "Epoch [110/1000], Loss: 15000.9375\n",
            "Epoch [110/1000], Loss: 12037.0918\n",
            "Epoch [110/1000], Loss: 14790.7324\n",
            "Epoch [110/1000], Loss: 14207.0977\n",
            "Epoch [110/1000], Loss: 14153.5312\n",
            "Epoch [110/1000], Loss: 10801.082\n",
            "Epoch [120/1000], Loss: 10966.6699\n",
            "Epoch [120/1000], Loss: 14091.5547\n",
            "Epoch [120/1000], Loss: 12187.0947\n",
            "Epoch [120/1000], Loss: 9649.8252\n",
            "Epoch [120/1000], Loss: 11171.6475\n",
            "Epoch [120/1000], Loss: 12760.6543\n",
            "Epoch [120/1000], Loss: 12862.2002\n",
            "Epoch [120/1000], Loss: 32555.5371\n",
            "Epoch [130/1000], Loss: 13914.3643\n",
            "Epoch [130/1000], Loss: 11483.9326\n",
            "Epoch [130/1000], Loss: 10265.5449\n",
            "Epoch [130/1000], Loss: 12651.5625\n",
            "Epoch [130/1000], Loss: 9005.8125\n",
            "Epoch [130/1000], Loss: 9608.8115\n",
            "Epoch [130/1000], Loss: 10144.4355\n",
            "Epoch [130/1000], Loss: 6814.9902\n",
            "Epoch [140/1000], Loss: 11577.7344\n",
            "Epoch [140/1000], Loss: 8377.207\n",
            "Epoch [140/1000], Loss: 11763.7998\n",
            "Epoch [140/1000], Loss: 6811.4707\n",
            "Epoch [140/1000], Loss: 10124.3994\n",
            "Epoch [140/1000], Loss: 13049.1943\n",
            "Epoch [140/1000], Loss: 7490.1973\n",
            "Epoch [140/1000], Loss: 15100.1084\n",
            "Epoch [150/1000], Loss: 9444.5107\n",
            "Epoch [150/1000], Loss: 9861.6504\n",
            "Epoch [150/1000], Loss: 9419.5889\n",
            "Epoch [150/1000], Loss: 9469.8252\n",
            "Epoch [150/1000], Loss: 8133.4609\n",
            "Epoch [150/1000], Loss: 8821.5498\n",
            "Epoch [150/1000], Loss: 6842.5879\n",
            "Epoch [150/1000], Loss: 18311.8086\n",
            "Epoch [160/1000], Loss: 3864.8042\n",
            "Epoch [160/1000], Loss: 7328.5215\n",
            "Epoch [160/1000], Loss: 7993.1963\n",
            "Epoch [160/1000], Loss: 9152.624\n",
            "Epoch [160/1000], Loss: 9505.1309\n",
            "Epoch [160/1000], Loss: 7549.3447\n",
            "Epoch [160/1000], Loss: 10899.3271\n",
            "Epoch [160/1000], Loss: 9218.8682\n",
            "Epoch [170/1000], Loss: 7794.8682\n",
            "Epoch [170/1000], Loss: 6050.6025\n",
            "Epoch [170/1000], Loss: 9844.3408\n",
            "Epoch [170/1000], Loss: 6780.4067\n",
            "Epoch [170/1000], Loss: 4222.4551\n",
            "Epoch [170/1000], Loss: 10080.0498\n",
            "Epoch [170/1000], Loss: 6592.75\n",
            "Epoch [170/1000], Loss: 5376.2583\n",
            "Epoch [180/1000], Loss: 7212.8071\n",
            "Epoch [180/1000], Loss: 5819.4204\n",
            "Epoch [180/1000], Loss: 4804.793\n",
            "Epoch [180/1000], Loss: 8755.5684\n",
            "Epoch [180/1000], Loss: 5032.0649\n",
            "Epoch [180/1000], Loss: 7775.8716\n",
            "Epoch [180/1000], Loss: 7079.9053\n",
            "Epoch [180/1000], Loss: 13049.5195\n",
            "Epoch [190/1000], Loss: 6527.4849\n",
            "Epoch [190/1000], Loss: 5424.2275\n",
            "Epoch [190/1000], Loss: 6595.8262\n",
            "Epoch [190/1000], Loss: 5778.7646\n",
            "Epoch [190/1000], Loss: 5329.3174\n",
            "Epoch [190/1000], Loss: 7532.73\n",
            "Epoch [190/1000], Loss: 5858.1138\n",
            "Epoch [190/1000], Loss: 4059.8467\n",
            "Epoch [200/1000], Loss: 8525.0781\n",
            "Epoch [200/1000], Loss: 4083.8127\n",
            "Epoch [200/1000], Loss: 6689.751\n",
            "Epoch [200/1000], Loss: 6621.8267\n",
            "Epoch [200/1000], Loss: 4702.8174\n",
            "Epoch [200/1000], Loss: 3775.0381\n",
            "Epoch [200/1000], Loss: 5533.3813\n",
            "Epoch [200/1000], Loss: 6084.6875\n",
            "Epoch [210/1000], Loss: 6328.9517\n",
            "Epoch [210/1000], Loss: 4703.7153\n",
            "Epoch [210/1000], Loss: 4403.2837\n",
            "Epoch [210/1000], Loss: 5787.1011\n",
            "Epoch [210/1000], Loss: 5869.3115\n",
            "Epoch [210/1000], Loss: 4831.9507\n",
            "Epoch [210/1000], Loss: 5889.5762\n",
            "Epoch [210/1000], Loss: 514.347\n",
            "Epoch [220/1000], Loss: 4787.3125\n",
            "Epoch [220/1000], Loss: 5799.9331\n",
            "Epoch [220/1000], Loss: 4536.3589\n",
            "Epoch [220/1000], Loss: 4605.0156\n",
            "Epoch [220/1000], Loss: 5743.6226\n",
            "Epoch [220/1000], Loss: 5722.0625\n",
            "Epoch [220/1000], Loss: 4348.0361\n",
            "Epoch [220/1000], Loss: 6202.5234\n",
            "Epoch [230/1000], Loss: 4334.5488\n",
            "Epoch [230/1000], Loss: 5380.8271\n",
            "Epoch [230/1000], Loss: 4598.8564\n",
            "Epoch [230/1000], Loss: 6312.249\n",
            "Epoch [230/1000], Loss: 3322.4202\n",
            "Epoch [230/1000], Loss: 5296.1948\n",
            "Epoch [230/1000], Loss: 4758.29\n",
            "Epoch [230/1000], Loss: 2798.6121\n",
            "Epoch [240/1000], Loss: 4965.4434\n",
            "Epoch [240/1000], Loss: 5624.75\n",
            "Epoch [240/1000], Loss: 4710.8843\n",
            "Epoch [240/1000], Loss: 4680.1572\n",
            "Epoch [240/1000], Loss: 3190.8481\n",
            "Epoch [240/1000], Loss: 4833.9277\n",
            "Epoch [240/1000], Loss: 4421.3159\n",
            "Epoch [240/1000], Loss: 8517.7627\n",
            "Epoch [250/1000], Loss: 5082.4878\n",
            "Epoch [250/1000], Loss: 4756.4214\n",
            "Epoch [250/1000], Loss: 4531.8569\n",
            "Epoch [250/1000], Loss: 4826.8242\n",
            "Epoch [250/1000], Loss: 4334.5293\n",
            "Epoch [250/1000], Loss: 3811.7148\n",
            "Epoch [250/1000], Loss: 4587.3828\n",
            "Epoch [250/1000], Loss: 1311.9858\n",
            "Epoch [260/1000], Loss: 5055.1431\n",
            "Epoch [260/1000], Loss: 5127.7095\n",
            "Epoch [260/1000], Loss: 4948.2266\n",
            "Epoch [260/1000], Loss: 3311.6831\n",
            "Epoch [260/1000], Loss: 3866.342\n",
            "Epoch [260/1000], Loss: 4693.6948\n",
            "Epoch [260/1000], Loss: 4248.2573\n",
            "Epoch [260/1000], Loss: 2100.9556\n",
            "Epoch [270/1000], Loss: 4812.1807\n",
            "Epoch [270/1000], Loss: 3311.5112\n",
            "Epoch [270/1000], Loss: 5421.5342\n",
            "Epoch [270/1000], Loss: 5044.3882\n",
            "Epoch [270/1000], Loss: 2554.8228\n",
            "Epoch [270/1000], Loss: 5004.3071\n",
            "Epoch [270/1000], Loss: 4463.728\n",
            "Epoch [270/1000], Loss: 4301.0596\n",
            "Epoch [280/1000], Loss: 5645.5234\n",
            "Epoch [280/1000], Loss: 4866.1304\n",
            "Epoch [280/1000], Loss: 3676.0435\n",
            "Epoch [280/1000], Loss: 3891.2422\n",
            "Epoch [280/1000], Loss: 4157.2339\n",
            "Epoch [280/1000], Loss: 4556.1274\n",
            "Epoch [280/1000], Loss: 3620.7588\n",
            "Epoch [280/1000], Loss: 1293.49\n",
            "Epoch [290/1000], Loss: 4601.8066\n",
            "Epoch [290/1000], Loss: 3301.1531\n",
            "Epoch [290/1000], Loss: 4669.749\n",
            "Epoch [290/1000], Loss: 3797.4792\n",
            "Epoch [290/1000], Loss: 3781.8862\n",
            "Epoch [290/1000], Loss: 5238.2876\n",
            "Epoch [290/1000], Loss: 4310.3223\n",
            "Epoch [290/1000], Loss: 7442.3022\n",
            "Epoch [300/1000], Loss: 5033.7583\n",
            "Epoch [300/1000], Loss: 3408.7639\n",
            "Epoch [300/1000], Loss: 4042.8037\n",
            "Epoch [300/1000], Loss: 4982.7515\n",
            "Epoch [300/1000], Loss: 3460.6223\n",
            "Epoch [300/1000], Loss: 4469.437\n",
            "Epoch [300/1000], Loss: 4265.8149\n",
            "Epoch [300/1000], Loss: 4213.9951\n",
            "Epoch [310/1000], Loss: 5029.2993\n",
            "Epoch [310/1000], Loss: 3328.7561\n",
            "Epoch [310/1000], Loss: 3802.4539\n",
            "Epoch [310/1000], Loss: 4152.7705\n",
            "Epoch [310/1000], Loss: 3887.1038\n",
            "Epoch [310/1000], Loss: 4784.9341\n",
            "Epoch [310/1000], Loss: 4325.0137\n",
            "Epoch [310/1000], Loss: 6699.9375\n",
            "Epoch [320/1000], Loss: 4401.9473\n",
            "Epoch [320/1000], Loss: 3406.6467\n",
            "Epoch [320/1000], Loss: 4449.5864\n",
            "Epoch [320/1000], Loss: 4473.4639\n",
            "Epoch [320/1000], Loss: 3987.6084\n",
            "Epoch [320/1000], Loss: 4081.7112\n",
            "Epoch [320/1000], Loss: 4506.6704\n",
            "Epoch [320/1000], Loss: 3659.8855\n",
            "Epoch [330/1000], Loss: 4310.6367\n",
            "Epoch [330/1000], Loss: 3107.78\n",
            "Epoch [330/1000], Loss: 3647.3086\n",
            "Epoch [330/1000], Loss: 3215.198\n",
            "Epoch [330/1000], Loss: 4528.4409\n",
            "Epoch [330/1000], Loss: 5318.6685\n",
            "Epoch [330/1000], Loss: 5046.6113\n",
            "Epoch [330/1000], Loss: 2294.3037\n",
            "Epoch [340/1000], Loss: 3616.8154\n",
            "Epoch [340/1000], Loss: 5212.1357\n",
            "Epoch [340/1000], Loss: 4529.02\n",
            "Epoch [340/1000], Loss: 5614.6675\n",
            "Epoch [340/1000], Loss: 3568.3867\n",
            "Epoch [340/1000], Loss: 3036.5234\n",
            "Epoch [340/1000], Loss: 3506.4961\n",
            "Epoch [340/1000], Loss: 631.613\n",
            "Epoch [350/1000], Loss: 4124.415\n",
            "Epoch [350/1000], Loss: 3740.3721\n",
            "Epoch [350/1000], Loss: 4348.4531\n",
            "Epoch [350/1000], Loss: 4410.2563\n",
            "Epoch [350/1000], Loss: 4117.917\n",
            "Epoch [350/1000], Loss: 3868.1021\n",
            "Epoch [350/1000], Loss: 3937.2725\n",
            "Epoch [350/1000], Loss: 6414.6738\n",
            "Epoch [360/1000], Loss: 5145.7188\n",
            "Epoch [360/1000], Loss: 3302.3503\n",
            "Epoch [360/1000], Loss: 4968.1396\n",
            "Epoch [360/1000], Loss: 3504.2646\n",
            "Epoch [360/1000], Loss: 4164.0005\n",
            "Epoch [360/1000], Loss: 3504.2432\n",
            "Epoch [360/1000], Loss: 3735.4187\n",
            "Epoch [360/1000], Loss: 7266.5151\n",
            "Epoch [370/1000], Loss: 4729.4653\n",
            "Epoch [370/1000], Loss: 3812.1309\n",
            "Epoch [370/1000], Loss: 5066.3604\n",
            "Epoch [370/1000], Loss: 3866.071\n",
            "Epoch [370/1000], Loss: 3224.5273\n",
            "Epoch [370/1000], Loss: 4499.0356\n",
            "Epoch [370/1000], Loss: 3178.9614\n",
            "Epoch [370/1000], Loss: 3608.9907\n",
            "Epoch [380/1000], Loss: 4490.9795\n",
            "Epoch [380/1000], Loss: 4050.2551\n",
            "Epoch [380/1000], Loss: 4038.573\n",
            "Epoch [380/1000], Loss: 4905.561\n",
            "Epoch [380/1000], Loss: 2750.9255\n",
            "Epoch [380/1000], Loss: 3624.2114\n",
            "Epoch [380/1000], Loss: 4139.9912\n",
            "Epoch [380/1000], Loss: 7145.2905\n",
            "Epoch [390/1000], Loss: 3427.3137\n",
            "Epoch [390/1000], Loss: 3845.9636\n",
            "Epoch [390/1000], Loss: 4598.2393\n",
            "Epoch [390/1000], Loss: 4168.2705\n",
            "Epoch [390/1000], Loss: 4998.1938\n",
            "Epoch [390/1000], Loss: 3237.6936\n",
            "Epoch [390/1000], Loss: 3729.7578\n",
            "Epoch [390/1000], Loss: 4431.4937\n",
            "Epoch [400/1000], Loss: 4705.4292\n",
            "Epoch [400/1000], Loss: 2789.4768\n",
            "Epoch [400/1000], Loss: 3741.657\n",
            "Epoch [400/1000], Loss: 5121.6211\n",
            "Epoch [400/1000], Loss: 3569.9871\n",
            "Epoch [400/1000], Loss: 4753.3882\n",
            "Epoch [400/1000], Loss: 3246.3704\n",
            "Epoch [400/1000], Loss: 3132.3298\n",
            "Epoch [410/1000], Loss: 5034.8613\n",
            "Epoch [410/1000], Loss: 3481.7056\n",
            "Epoch [410/1000], Loss: 4597.3447\n",
            "Epoch [410/1000], Loss: 4188.522\n",
            "Epoch [410/1000], Loss: 3707.8467\n",
            "Epoch [410/1000], Loss: 3003.8855\n",
            "Epoch [410/1000], Loss: 3669.4868\n",
            "Epoch [410/1000], Loss: 4576.1655\n",
            "Epoch [420/1000], Loss: 4204.0874\n",
            "Epoch [420/1000], Loss: 3934.967\n",
            "Epoch [420/1000], Loss: 4276.0688\n",
            "Epoch [420/1000], Loss: 3665.2332\n",
            "Epoch [420/1000], Loss: 3039.5881\n",
            "Epoch [420/1000], Loss: 4848.9521\n",
            "Epoch [420/1000], Loss: 3739.4756\n",
            "Epoch [420/1000], Loss: 1753.7397\n",
            "Epoch [430/1000], Loss: 3630.1118\n",
            "Epoch [430/1000], Loss: 3450.9287\n",
            "Epoch [430/1000], Loss: 4505.7476\n",
            "Epoch [430/1000], Loss: 3641.9067\n",
            "Epoch [430/1000], Loss: 4155.6206\n",
            "Epoch [430/1000], Loss: 4119.043\n",
            "Epoch [430/1000], Loss: 4083.8125\n",
            "Epoch [430/1000], Loss: 1304.8958\n",
            "Epoch [440/1000], Loss: 2864.499\n",
            "Epoch [440/1000], Loss: 4226.6675\n",
            "Epoch [440/1000], Loss: 4139.5024\n",
            "Epoch [440/1000], Loss: 4434.2842\n",
            "Epoch [440/1000], Loss: 4389.0376\n",
            "Epoch [440/1000], Loss: 3878.8337\n",
            "Epoch [440/1000], Loss: 3380.8325\n",
            "Epoch [440/1000], Loss: 3626.1843\n",
            "Epoch [450/1000], Loss: 3050.2751\n",
            "Epoch [450/1000], Loss: 4527.8838\n",
            "Epoch [450/1000], Loss: 4036.5034\n",
            "Epoch [450/1000], Loss: 4833.0044\n",
            "Epoch [450/1000], Loss: 3517.4092\n",
            "Epoch [450/1000], Loss: 3623.2993\n",
            "Epoch [450/1000], Loss: 3449.3538\n",
            "Epoch [450/1000], Loss: 5926.5732\n",
            "Epoch [460/1000], Loss: 3490.4924\n",
            "Epoch [460/1000], Loss: 4082.5156\n",
            "Epoch [460/1000], Loss: 3926.4895\n",
            "Epoch [460/1000], Loss: 3987.0173\n",
            "Epoch [460/1000], Loss: 2965.3337\n",
            "Epoch [460/1000], Loss: 4375.4927\n",
            "Epoch [460/1000], Loss: 4041.4912\n",
            "Epoch [460/1000], Loss: 6584.8594\n",
            "Epoch [470/1000], Loss: 3770.9402\n",
            "Epoch [470/1000], Loss: 4493.9575\n",
            "Epoch [470/1000], Loss: 3879.3843\n",
            "Epoch [470/1000], Loss: 3986.6868\n",
            "Epoch [470/1000], Loss: 3119.6946\n",
            "Epoch [470/1000], Loss: 3336.7705\n",
            "Epoch [470/1000], Loss: 4168.6982\n",
            "Epoch [470/1000], Loss: 6004.3472\n",
            "Epoch [480/1000], Loss: 3394.3262\n",
            "Epoch [480/1000], Loss: 5002.5449\n",
            "Epoch [480/1000], Loss: 3489.1814\n",
            "Epoch [480/1000], Loss: 3110.8733\n",
            "Epoch [480/1000], Loss: 4312.3032\n",
            "Epoch [480/1000], Loss: 3902.7473\n",
            "Epoch [480/1000], Loss: 3609.343\n",
            "Epoch [480/1000], Loss: 2496.668\n",
            "Epoch [490/1000], Loss: 2843.8286\n",
            "Epoch [490/1000], Loss: 3660.2725\n",
            "Epoch [490/1000], Loss: 3047.9475\n",
            "Epoch [490/1000], Loss: 5400.0649\n",
            "Epoch [490/1000], Loss: 3486.6406\n",
            "Epoch [490/1000], Loss: 4463.9912\n",
            "Epoch [490/1000], Loss: 3705.8865\n",
            "Epoch [490/1000], Loss: 3730.9753\n",
            "Epoch [500/1000], Loss: 3959.3801\n",
            "Epoch [500/1000], Loss: 3728.5122\n",
            "Epoch [500/1000], Loss: 3899.5981\n",
            "Epoch [500/1000], Loss: 3856.2488\n",
            "Epoch [500/1000], Loss: 3483.7266\n",
            "Epoch [500/1000], Loss: 3818.467\n",
            "Epoch [500/1000], Loss: 3701.7827\n",
            "Epoch [500/1000], Loss: 4364.6479\n",
            "Epoch [510/1000], Loss: 3852.6636\n",
            "Epoch [510/1000], Loss: 4643.1499\n",
            "Epoch [510/1000], Loss: 3424.2422\n",
            "Epoch [510/1000], Loss: 3726.4453\n",
            "Epoch [510/1000], Loss: 3189.1421\n",
            "Epoch [510/1000], Loss: 3335.4102\n",
            "Epoch [510/1000], Loss: 4343.0347\n",
            "Epoch [510/1000], Loss: 1065.0107\n",
            "Epoch [520/1000], Loss: 2661.698\n",
            "Epoch [520/1000], Loss: 3498.113\n",
            "Epoch [520/1000], Loss: 4915.021\n",
            "Epoch [520/1000], Loss: 4261.5244\n",
            "Epoch [520/1000], Loss: 3458.5405\n",
            "Epoch [520/1000], Loss: 3786.7517\n",
            "Epoch [520/1000], Loss: 3523.3296\n",
            "Epoch [520/1000], Loss: 6039.7188\n",
            "Epoch [530/1000], Loss: 3857.7581\n",
            "Epoch [530/1000], Loss: 3605.6367\n",
            "Epoch [530/1000], Loss: 3785.1155\n",
            "Epoch [530/1000], Loss: 4501.27\n",
            "Epoch [530/1000], Loss: 3534.293\n",
            "Epoch [530/1000], Loss: 3512.9873\n",
            "Epoch [530/1000], Loss: 3056.7415\n",
            "Epoch [530/1000], Loss: 8267.1191\n",
            "Epoch [540/1000], Loss: 2879.7949\n",
            "Epoch [540/1000], Loss: 3564.3618\n",
            "Epoch [540/1000], Loss: 3516.3215\n",
            "Epoch [540/1000], Loss: 3709.8074\n",
            "Epoch [540/1000], Loss: 3889.8911\n",
            "Epoch [540/1000], Loss: 4213.3813\n",
            "Epoch [540/1000], Loss: 3810.9995\n",
            "Epoch [540/1000], Loss: 10417.373\n",
            "Epoch [550/1000], Loss: 3770.0559\n",
            "Epoch [550/1000], Loss: 4483.3413\n",
            "Epoch [550/1000], Loss: 3975.0664\n",
            "Epoch [550/1000], Loss: 3392.0862\n",
            "Epoch [550/1000], Loss: 2805.6924\n",
            "Epoch [550/1000], Loss: 3842.6536\n",
            "Epoch [550/1000], Loss: 3600.6338\n",
            "Epoch [550/1000], Loss: 3441.53\n",
            "Epoch [560/1000], Loss: 3310.1262\n",
            "Epoch [560/1000], Loss: 2997.6128\n",
            "Epoch [560/1000], Loss: 3815.9207\n",
            "Epoch [560/1000], Loss: 2996.0598\n",
            "Epoch [560/1000], Loss: 4686.9229\n",
            "Epoch [560/1000], Loss: 4346.5815\n",
            "Epoch [560/1000], Loss: 3716.4644\n",
            "Epoch [560/1000], Loss: 1573.7245\n",
            "Epoch [570/1000], Loss: 4849.123\n",
            "Epoch [570/1000], Loss: 3463.1133\n",
            "Epoch [570/1000], Loss: 3146.2546\n",
            "Epoch [570/1000], Loss: 3389.7927\n",
            "Epoch [570/1000], Loss: 3622.6309\n",
            "Epoch [570/1000], Loss: 3994.3862\n",
            "Epoch [570/1000], Loss: 3222.6399\n",
            "Epoch [570/1000], Loss: 2740.0708\n",
            "Epoch [580/1000], Loss: 4378.4097\n",
            "Epoch [580/1000], Loss: 2574.7322\n",
            "Epoch [580/1000], Loss: 3972.3711\n",
            "Epoch [580/1000], Loss: 3670.2993\n",
            "Epoch [580/1000], Loss: 3972.6401\n",
            "Epoch [580/1000], Loss: 4234.8872\n",
            "Epoch [580/1000], Loss: 2612.804\n",
            "Epoch [580/1000], Loss: 5588.2646\n",
            "Epoch [590/1000], Loss: 3118.1584\n",
            "Epoch [590/1000], Loss: 3603.6926\n",
            "Epoch [590/1000], Loss: 3145.5369\n",
            "Epoch [590/1000], Loss: 5238.3232\n",
            "Epoch [590/1000], Loss: 3394.8018\n",
            "Epoch [590/1000], Loss: 2868.115\n",
            "Epoch [590/1000], Loss: 4023.6633\n",
            "Epoch [590/1000], Loss: 3860.5205\n",
            "Epoch [600/1000], Loss: 3732.4448\n",
            "Epoch [600/1000], Loss: 3532.0059\n",
            "Epoch [600/1000], Loss: 3855.1316\n",
            "Epoch [600/1000], Loss: 3834.1365\n",
            "Epoch [600/1000], Loss: 3338.1487\n",
            "Epoch [600/1000], Loss: 3361.3936\n",
            "Epoch [600/1000], Loss: 3477.0457\n",
            "Epoch [600/1000], Loss: 6339.8076\n",
            "Epoch [610/1000], Loss: 3219.2615\n",
            "Epoch [610/1000], Loss: 2807.8728\n",
            "Epoch [610/1000], Loss: 3093.1675\n",
            "Epoch [610/1000], Loss: 4940.2856\n",
            "Epoch [610/1000], Loss: 4634.3623\n",
            "Epoch [610/1000], Loss: 2912.5188\n",
            "Epoch [610/1000], Loss: 3592.7153\n",
            "Epoch [610/1000], Loss: 3486.3438\n",
            "Epoch [620/1000], Loss: 3139.9062\n",
            "Epoch [620/1000], Loss: 3904.7783\n",
            "Epoch [620/1000], Loss: 4380.7759\n",
            "Epoch [620/1000], Loss: 3452.2031\n",
            "Epoch [620/1000], Loss: 3459.7622\n",
            "Epoch [620/1000], Loss: 3291.7961\n",
            "Epoch [620/1000], Loss: 3496.3237\n",
            "Epoch [620/1000], Loss: 2886.4644\n",
            "Epoch [630/1000], Loss: 4029.3298\n",
            "Epoch [630/1000], Loss: 3005.7649\n",
            "Epoch [630/1000], Loss: 3445.4299\n",
            "Epoch [630/1000], Loss: 4167.4082\n",
            "Epoch [630/1000], Loss: 3006.998\n",
            "Epoch [630/1000], Loss: 3467.5273\n",
            "Epoch [630/1000], Loss: 3642.1206\n",
            "Epoch [630/1000], Loss: 7311.0195\n",
            "Epoch [640/1000], Loss: 3730.5715\n",
            "Epoch [640/1000], Loss: 3185.9214\n",
            "Epoch [640/1000], Loss: 3449.2373\n",
            "Epoch [640/1000], Loss: 3610.4736\n",
            "Epoch [640/1000], Loss: 3282.0193\n",
            "Epoch [640/1000], Loss: 4317.2734\n",
            "Epoch [640/1000], Loss: 3357.6987\n",
            "Epoch [640/1000], Loss: 2789.1277\n",
            "Epoch [650/1000], Loss: 3063.2424\n",
            "Epoch [650/1000], Loss: 3963.2998\n",
            "Epoch [650/1000], Loss: 3241.5911\n",
            "Epoch [650/1000], Loss: 3429.5361\n",
            "Epoch [650/1000], Loss: 4056.189\n",
            "Epoch [650/1000], Loss: 3744.3867\n",
            "Epoch [650/1000], Loss: 3180.8894\n",
            "Epoch [650/1000], Loss: 5446.6309\n",
            "Epoch [660/1000], Loss: 3234.0081\n",
            "Epoch [660/1000], Loss: 4192.4629\n",
            "Epoch [660/1000], Loss: 3478.6855\n",
            "Epoch [660/1000], Loss: 3065.8906\n",
            "Epoch [660/1000], Loss: 3274.0327\n",
            "Epoch [660/1000], Loss: 3626.95\n",
            "Epoch [660/1000], Loss: 3679.1062\n",
            "Epoch [660/1000], Loss: 5722.9873\n",
            "Epoch [670/1000], Loss: 2640.6833\n",
            "Epoch [670/1000], Loss: 3312.7874\n",
            "Epoch [670/1000], Loss: 3625.6914\n",
            "Epoch [670/1000], Loss: 3348.6409\n",
            "Epoch [670/1000], Loss: 3348.6611\n",
            "Epoch [670/1000], Loss: 3365.2629\n",
            "Epoch [670/1000], Loss: 4542.1167\n",
            "Epoch [670/1000], Loss: 10220.873\n",
            "Epoch [680/1000], Loss: 2976.4749\n",
            "Epoch [680/1000], Loss: 3580.7817\n",
            "Epoch [680/1000], Loss: 3258.2126\n",
            "Epoch [680/1000], Loss: 3279.7917\n",
            "Epoch [680/1000], Loss: 4024.3586\n",
            "Epoch [680/1000], Loss: 3729.4692\n",
            "Epoch [680/1000], Loss: 3632.3525\n",
            "Epoch [680/1000], Loss: 3739.606\n",
            "Epoch [690/1000], Loss: 4858.0229\n",
            "Epoch [690/1000], Loss: 3670.9309\n",
            "Epoch [690/1000], Loss: 3595.3887\n",
            "Epoch [690/1000], Loss: 3147.4905\n",
            "Epoch [690/1000], Loss: 3072.8855\n",
            "Epoch [690/1000], Loss: 2669.3916\n",
            "Epoch [690/1000], Loss: 3202.0898\n",
            "Epoch [690/1000], Loss: 6697.4326\n",
            "Epoch [700/1000], Loss: 3869.6328\n",
            "Epoch [700/1000], Loss: 3171.7366\n",
            "Epoch [700/1000], Loss: 3059.7502\n",
            "Epoch [700/1000], Loss: 3399.9656\n",
            "Epoch [700/1000], Loss: 3898.8679\n",
            "Epoch [700/1000], Loss: 2686.0288\n",
            "Epoch [700/1000], Loss: 4213.7065\n",
            "Epoch [700/1000], Loss: 3628.5664\n",
            "Epoch [710/1000], Loss: 3979.8853\n",
            "Epoch [710/1000], Loss: 3189.5149\n",
            "Epoch [710/1000], Loss: 3122.6797\n",
            "Epoch [710/1000], Loss: 4120.7231\n",
            "Epoch [710/1000], Loss: 2802.0061\n",
            "Epoch [710/1000], Loss: 3364.6799\n",
            "Epoch [710/1000], Loss: 3599.2791\n",
            "Epoch [710/1000], Loss: 4172.0894\n",
            "Epoch [720/1000], Loss: 3427.4412\n",
            "Epoch [720/1000], Loss: 3250.855\n",
            "Epoch [720/1000], Loss: 4166.1421\n",
            "Epoch [720/1000], Loss: 4267.1099\n",
            "Epoch [720/1000], Loss: 3348.7495\n",
            "Epoch [720/1000], Loss: 3127.9331\n",
            "Epoch [720/1000], Loss: 2629.7671\n",
            "Epoch [720/1000], Loss: 2184.4807\n",
            "Epoch [730/1000], Loss: 2721.9399\n",
            "Epoch [730/1000], Loss: 3994.7649\n",
            "Epoch [730/1000], Loss: 3386.4155\n",
            "Epoch [730/1000], Loss: 3285.2112\n",
            "Epoch [730/1000], Loss: 3431.0349\n",
            "Epoch [730/1000], Loss: 3630.7239\n",
            "Epoch [730/1000], Loss: 3605.6943\n",
            "Epoch [730/1000], Loss: 3459.5828\n",
            "Epoch [740/1000], Loss: 3264.4131\n",
            "Epoch [740/1000], Loss: 3548.3604\n",
            "Epoch [740/1000], Loss: 2969.5759\n",
            "Epoch [740/1000], Loss: 4292.416\n",
            "Epoch [740/1000], Loss: 2822.5093\n",
            "Epoch [740/1000], Loss: 3476.2917\n",
            "Epoch [740/1000], Loss: 3367.5635\n",
            "Epoch [740/1000], Loss: 7421.8374\n",
            "Epoch [750/1000], Loss: 3478.7256\n",
            "Epoch [750/1000], Loss: 2886.094\n",
            "Epoch [750/1000], Loss: 3832.9944\n",
            "Epoch [750/1000], Loss: 3065.6653\n",
            "Epoch [750/1000], Loss: 3622.5764\n",
            "Epoch [750/1000], Loss: 2551.6721\n",
            "Epoch [750/1000], Loss: 4574.1597\n",
            "Epoch [750/1000], Loss: 1591.9595\n",
            "Epoch [760/1000], Loss: 3897.8093\n",
            "Epoch [760/1000], Loss: 2749.8013\n",
            "Epoch [760/1000], Loss: 3612.4119\n",
            "Epoch [760/1000], Loss: 3716.9285\n",
            "Epoch [760/1000], Loss: 2512.2524\n",
            "Epoch [760/1000], Loss: 3593.0625\n",
            "Epoch [760/1000], Loss: 3780.3149\n",
            "Epoch [760/1000], Loss: 2650.2349\n",
            "Epoch [770/1000], Loss: 2675.855\n",
            "Epoch [770/1000], Loss: 3252.053\n",
            "Epoch [770/1000], Loss: 4780.9165\n",
            "Epoch [770/1000], Loss: 2694.2568\n",
            "Epoch [770/1000], Loss: 3224.6597\n",
            "Epoch [770/1000], Loss: 3124.8525\n",
            "Epoch [770/1000], Loss: 3914.4817\n",
            "Epoch [770/1000], Loss: 4619.168\n",
            "Epoch [780/1000], Loss: 3449.9656\n",
            "Epoch [780/1000], Loss: 3415.6375\n",
            "Epoch [780/1000], Loss: 3838.8843\n",
            "Epoch [780/1000], Loss: 3510.5786\n",
            "Epoch [780/1000], Loss: 3157.3083\n",
            "Epoch [780/1000], Loss: 3400.959\n",
            "Epoch [780/1000], Loss: 2913.8142\n",
            "Epoch [780/1000], Loss: 3101.532\n",
            "Epoch [790/1000], Loss: 3015.2031\n",
            "Epoch [790/1000], Loss: 3658.498\n",
            "Epoch [790/1000], Loss: 2541.5452\n",
            "Epoch [790/1000], Loss: 3355.7869\n",
            "Epoch [790/1000], Loss: 2772.29\n",
            "Epoch [790/1000], Loss: 4804.3779\n",
            "Epoch [790/1000], Loss: 3564.3979\n",
            "Epoch [790/1000], Loss: 1359.1794\n",
            "Epoch [800/1000], Loss: 3063.5681\n",
            "Epoch [800/1000], Loss: 3841.739\n",
            "Epoch [800/1000], Loss: 2445.9189\n",
            "Epoch [800/1000], Loss: 3708.0718\n",
            "Epoch [800/1000], Loss: 4571.123\n",
            "Epoch [800/1000], Loss: 3275.2134\n",
            "Epoch [800/1000], Loss: 2650.9937\n",
            "Epoch [800/1000], Loss: 2872.9644\n",
            "Epoch [810/1000], Loss: 3458.3025\n",
            "Epoch [810/1000], Loss: 3669.3396\n",
            "Epoch [810/1000], Loss: 3280.9072\n",
            "Epoch [810/1000], Loss: 4577.9116\n",
            "Epoch [810/1000], Loss: 3096.8933\n",
            "Epoch [810/1000], Loss: 2378.8044\n",
            "Epoch [810/1000], Loss: 3136.8337\n",
            "Epoch [810/1000], Loss: 707.9179\n",
            "Epoch [820/1000], Loss: 3950.7344\n",
            "Epoch [820/1000], Loss: 2846.3918\n",
            "Epoch [820/1000], Loss: 3418.0854\n",
            "Epoch [820/1000], Loss: 3599.3481\n",
            "Epoch [820/1000], Loss: 3137.063\n",
            "Epoch [820/1000], Loss: 2720.5786\n",
            "Epoch [820/1000], Loss: 3710.6924\n",
            "Epoch [820/1000], Loss: 3357.1892\n",
            "Epoch [830/1000], Loss: 3337.5017\n",
            "Epoch [830/1000], Loss: 3355.5911\n",
            "Epoch [830/1000], Loss: 3173.1931\n",
            "Epoch [830/1000], Loss: 3714.1987\n",
            "Epoch [830/1000], Loss: 3395.1311\n",
            "Epoch [830/1000], Loss: 3122.0088\n",
            "Epoch [830/1000], Loss: 3194.8005\n",
            "Epoch [830/1000], Loss: 3530.4009\n",
            "Epoch [840/1000], Loss: 2409.2407\n",
            "Epoch [840/1000], Loss: 3118.3738\n",
            "Epoch [840/1000], Loss: 3313.218\n",
            "Epoch [840/1000], Loss: 2830.7202\n",
            "Epoch [840/1000], Loss: 4084.125\n",
            "Epoch [840/1000], Loss: 3051.2786\n",
            "Epoch [840/1000], Loss: 4597.5542\n",
            "Epoch [840/1000], Loss: 518.132\n",
            "Epoch [850/1000], Loss: 3426.0293\n",
            "Epoch [850/1000], Loss: 2990.665\n",
            "Epoch [850/1000], Loss: 3513.5293\n",
            "Epoch [850/1000], Loss: 4035.5337\n",
            "Epoch [850/1000], Loss: 3451.469\n",
            "Epoch [850/1000], Loss: 2520.4109\n",
            "Epoch [850/1000], Loss: 3369.4856\n",
            "Epoch [850/1000], Loss: 1029.0715\n",
            "Epoch [860/1000], Loss: 3480.4331\n",
            "Epoch [860/1000], Loss: 2663.2981\n",
            "Epoch [860/1000], Loss: 3697.4675\n",
            "Epoch [860/1000], Loss: 3053.9871\n",
            "Epoch [860/1000], Loss: 2750.0444\n",
            "Epoch [860/1000], Loss: 4237.6963\n",
            "Epoch [860/1000], Loss: 3253.4497\n",
            "Epoch [860/1000], Loss: 2782.5112\n",
            "Epoch [870/1000], Loss: 3117.5955\n",
            "Epoch [870/1000], Loss: 3304.4648\n",
            "Epoch [870/1000], Loss: 3094.5283\n",
            "Epoch [870/1000], Loss: 4075.8774\n",
            "Epoch [870/1000], Loss: 3422.5911\n",
            "Epoch [870/1000], Loss: 2791.4177\n",
            "Epoch [870/1000], Loss: 3297.9414\n",
            "Epoch [870/1000], Loss: 2364.0947\n",
            "Epoch [880/1000], Loss: 3611.27\n",
            "Epoch [880/1000], Loss: 2750.823\n",
            "Epoch [880/1000], Loss: 2707.437\n",
            "Epoch [880/1000], Loss: 4858.6943\n",
            "Epoch [880/1000], Loss: 3242.2812\n",
            "Epoch [880/1000], Loss: 3228.5312\n",
            "Epoch [880/1000], Loss: 2744.9568\n",
            "Epoch [880/1000], Loss: 570.4033\n",
            "Epoch [890/1000], Loss: 2038.4359\n",
            "Epoch [890/1000], Loss: 3433.5095\n",
            "Epoch [890/1000], Loss: 2666.2791\n",
            "Epoch [890/1000], Loss: 3936.4287\n",
            "Epoch [890/1000], Loss: 3951.5471\n",
            "Epoch [890/1000], Loss: 2871.0649\n",
            "Epoch [890/1000], Loss: 4040.3928\n",
            "Epoch [890/1000], Loss: 3155.3838\n",
            "Epoch [900/1000], Loss: 2869.5032\n",
            "Epoch [900/1000], Loss: 3344.4805\n",
            "Epoch [900/1000], Loss: 3625.4583\n",
            "Epoch [900/1000], Loss: 3108.7688\n",
            "Epoch [900/1000], Loss: 2499.8191\n",
            "Epoch [900/1000], Loss: 3794.8394\n",
            "Epoch [900/1000], Loss: 3615.2188\n",
            "Epoch [900/1000], Loss: 3550.7488\n",
            "Epoch [910/1000], Loss: 2810.4253\n",
            "Epoch [910/1000], Loss: 4180.0586\n",
            "Epoch [910/1000], Loss: 3589.8567\n",
            "Epoch [910/1000], Loss: 3214.1423\n",
            "Epoch [910/1000], Loss: 2289.4727\n",
            "Epoch [910/1000], Loss: 3477.78\n",
            "Epoch [910/1000], Loss: 3156.7375\n",
            "Epoch [910/1000], Loss: 4668.415\n",
            "Epoch [920/1000], Loss: 3747.7949\n",
            "Epoch [920/1000], Loss: 2545.396\n",
            "Epoch [920/1000], Loss: 3510.8098\n",
            "Epoch [920/1000], Loss: 3028.7969\n",
            "Epoch [920/1000], Loss: 3541.8025\n",
            "Epoch [920/1000], Loss: 2642.8362\n",
            "Epoch [920/1000], Loss: 3673.6924\n",
            "Epoch [920/1000], Loss: 4031.5476\n",
            "Epoch [930/1000], Loss: 2468.1465\n",
            "Epoch [930/1000], Loss: 2230.8696\n",
            "Epoch [930/1000], Loss: 3420.2817\n",
            "Epoch [930/1000], Loss: 3715.0396\n",
            "Epoch [930/1000], Loss: 3337.8838\n",
            "Epoch [930/1000], Loss: 4201.7241\n",
            "Epoch [930/1000], Loss: 3183.1736\n",
            "Epoch [930/1000], Loss: 5418.6406\n",
            "Epoch [940/1000], Loss: 3238.8147\n",
            "Epoch [940/1000], Loss: 3814.533\n",
            "Epoch [940/1000], Loss: 2417.9421\n",
            "Epoch [940/1000], Loss: 2435.6531\n",
            "Epoch [940/1000], Loss: 2855.4619\n",
            "Epoch [940/1000], Loss: 3401.7031\n",
            "Epoch [940/1000], Loss: 4519.856\n",
            "Epoch [940/1000], Loss: 2338.6812\n",
            "Epoch [950/1000], Loss: 2633.4387\n",
            "Epoch [950/1000], Loss: 3879.9587\n",
            "Epoch [950/1000], Loss: 2885.7275\n",
            "Epoch [950/1000], Loss: 2633.9324\n",
            "Epoch [950/1000], Loss: 3376.6367\n",
            "Epoch [950/1000], Loss: 4340.5068\n",
            "Epoch [950/1000], Loss: 2692.1538\n",
            "Epoch [950/1000], Loss: 5273.8589\n",
            "Epoch [960/1000], Loss: 2681.0813\n",
            "Epoch [960/1000], Loss: 2869.655\n",
            "Epoch [960/1000], Loss: 3631.0793\n",
            "Epoch [960/1000], Loss: 2779.2979\n",
            "Epoch [960/1000], Loss: 3138.3884\n",
            "Epoch [960/1000], Loss: 4806.2017\n",
            "Epoch [960/1000], Loss: 2765.5798\n",
            "Epoch [960/1000], Loss: 632.9255\n",
            "Epoch [970/1000], Loss: 3736.27\n",
            "Epoch [970/1000], Loss: 3300.7764\n",
            "Epoch [970/1000], Loss: 3099.04\n",
            "Epoch [970/1000], Loss: 3212.8831\n",
            "Epoch [970/1000], Loss: 3505.5195\n",
            "Epoch [970/1000], Loss: 3224.835\n",
            "Epoch [970/1000], Loss: 2322.5266\n",
            "Epoch [970/1000], Loss: 4141.9272\n",
            "Epoch [980/1000], Loss: 2887.1411\n",
            "Epoch [980/1000], Loss: 2773.2913\n",
            "Epoch [980/1000], Loss: 3043.4556\n",
            "Epoch [980/1000], Loss: 3285.9111\n",
            "Epoch [980/1000], Loss: 3416.7302\n",
            "Epoch [980/1000], Loss: 3471.8667\n",
            "Epoch [980/1000], Loss: 3497.96\n",
            "Epoch [980/1000], Loss: 3655.5535\n",
            "Epoch [990/1000], Loss: 3099.1809\n",
            "Epoch [990/1000], Loss: 3931.6602\n",
            "Epoch [990/1000], Loss: 3342.7473\n",
            "Epoch [990/1000], Loss: 3529.9033\n",
            "Epoch [990/1000], Loss: 2666.7969\n",
            "Epoch [990/1000], Loss: 2889.8328\n",
            "Epoch [990/1000], Loss: 3020.5642\n",
            "Epoch [990/1000], Loss: 1119.5321\n",
            "Epoch [1000/1000], Loss: 2443.25\n",
            "Epoch [1000/1000], Loss: 3723.2539\n",
            "Epoch [1000/1000], Loss: 2772.5024\n",
            "Epoch [1000/1000], Loss: 3417.3247\n",
            "Epoch [1000/1000], Loss: 3026.2673\n",
            "Epoch [1000/1000], Loss: 3090.7656\n",
            "Epoch [1000/1000], Loss: 3772.9365\n",
            "Epoch [1000/1000], Loss: 4155.5713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6T5F5-XcvvV",
        "outputId": "2758f677-c8be-4d89-c8db-a32877d51687"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2937.7928466796875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ELvxA2pocy1g",
        "outputId": "67b2ef79-6b13-475b-f270-580415b8a070"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Predicted  Actual\n",
              "0   161.099701   219.0\n",
              "1   164.992020    70.0\n",
              "2   155.415451   202.0\n",
              "3   282.019897   230.0\n",
              "4   144.113434   111.0\n",
              "..         ...     ...\n",
              "84  121.455658   153.0\n",
              "85   92.503250    98.0\n",
              "86   74.003494    37.0\n",
              "87   73.831207    63.0\n",
              "88  141.804886   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e62aca5f-7e16-4d1a-addc-100d6e2f9529\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>161.099701</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>164.992020</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>155.415451</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>282.019897</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>144.113434</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>121.455658</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>92.503250</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>74.003494</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>73.831207</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>141.804886</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e62aca5f-7e16-4d1a-addc-100d6e2f9529')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e62aca5f-7e16-4d1a-addc-100d6e2f9529 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e62aca5f-7e16-4d1a-addc-100d6e2f9529');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_617bd03f-33b8-4ccd-bad0-1981b3ec1b3d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_617bd03f-33b8-4ccd-bad0-1981b3ec1b3d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          174.06443786621094,\n          116.57258605957031,\n          171.70826721191406\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After increasing the epochs to 1000, the average MSE on the test set is 2937.79. Previously, with 100 epochs, the average MSE was 22991.63. This shows a significant improvement in the model's performance on the test set with more training."
      ],
      "metadata": {
        "id": "Oxfj73nzdCKz"
      }
    }
  ]
}